{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPQmfg066/xPYhmqkG+rYdA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gjeffroy/LLM_finetuning/blob/main/Eval_untrained_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1DRM_pVfBJL"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth \"xformers==0.0.28.post2\"\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Phi-3.5-mini-instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "id": "AM2Ni_BufNpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Infering"
      ],
      "metadata": {
        "id": "36rtrbFbh99a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# load df\n",
        "df = pd.read_csv('df_reponse_orientees_mistral.csv', sep = ';')"
      ],
      "metadata": {
        "id": "gMDYj3Fbwkw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "eos_token_id = tokenizer.convert_tokens_to_ids(\"<|end|>\")\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"phi-3\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
      ],
      "metadata": {
        "id": "ntCbYlfE0Iiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_assistant_response(generated_text):\n",
        "    # Ensure the input is a string (if it's in a list or batch, adjust accordingly)\n",
        "    if isinstance(generated_text, list):\n",
        "        generated_text = generated_text[0]  # Take the first entry in case it's a batch\n",
        "\n",
        "    # Find where the assistant response starts and ends\n",
        "    start_token = \"<|assistant|>\"\n",
        "    end_token = \"<|end|>\"\n",
        "\n",
        "    # Find start and end indices for the assistant's response\n",
        "    start_idx = generated_text.find(start_token)\n",
        "    end_idx = generated_text.find(end_token, start_idx)\n",
        "\n",
        "    if start_idx == -1 or end_idx == -1:\n",
        "        return \"\"  # Return empty string if tokens aren't found\n",
        "\n",
        "    # Extract the assistant's response and strip any surrounding whitespace\n",
        "    return generated_text[start_idx + len(start_token):end_idx].strip()"
      ],
      "metadata": {
        "id": "fQu3nKb42Yv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(question):\n",
        "\n",
        "  system_message = \"\"\"\n",
        "    Tu es un assistant qui fournit des réponses en rapport avec le code d'urbanisme.\n",
        "    Si la question n'est pas orienté code de l'ubrannisme, repond que ce n'est pas ton domaine de connaissance.\n",
        "    Renvoi une reponse si la question concernent le code de l'urbanisme.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  messages = [\n",
        "      {\"from\": \"system\", \"value\": system_message },\n",
        "      {\"from\": \"human\", \"value\": question}\n",
        "  ]\n",
        "  inputs = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize = True,\n",
        "      add_generation_prompt = True, # Must add for generation\n",
        "      return_tensors = \"pt\",\n",
        "  ).to(\"cuda\")\n",
        "\n",
        "  outputs = model.generate(input_ids = inputs, max_new_tokens = 2000, use_cache = True, eos_token_id = eos_token_id)\n",
        "  return extract_assistant_response(tokenizer.batch_decode(outputs))"
      ],
      "metadata": {
        "id": "E55QRO-l1iNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not 'response_untrained' in df.columns:\n",
        "  df['response_untrained'] = pd.NA\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "sub_df = df[0:5]\n",
        "\n",
        "tic = datetime.now()\n",
        "for i in tqdm(range(len(sub_df))):\n",
        "  df.loc[i, 'response_untrained'] = infer(df.loc[i, 'question_orientée'])\n",
        "tac = datetime.now()\n",
        "print(tac - tic)\n",
        "print('average time: ' + str((tac - tic) / len(sub_df)))"
      ],
      "metadata": {
        "id": "_5lbnhFv2epp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}